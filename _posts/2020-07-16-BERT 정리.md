---
title: BERT 정리
tags: nlp, transformer, BERT
---
 

## Seq2seq + Attention 모델 메커니즘

Seq2seq 모델은 글자, 단어, 이미지의 feature 등의 아이템 시퀀스를 입력으로 받아 또 다른 아이템의 시퀀스를 출력

신경망 기계 번역의 경우에 대해서 본다면, 입력은 일련의 단어로 이루어진 sequence 이며 맨 앞 단어부터 차례대로 모델에서 처리됨

출력은 비슷한 형태의 그러나 다른 언어로의 단어 sequence가 나오게 됨

이 모델 내에는 하나의 encoder와 하나의 decoder로 이루어져 있음

encoder는 입력의 각 아이템을 처리하여 거기서 정보
(Context)를 추출한 후 그것을 하나의 벡터로 만들어냄

입력의 모든 단어에 대한 처리가 끝난 후 encoder는 context를 decoder에게 보내 출력할 아이템이 하나씩 선택됨

기계 번역의 경우 context가 하나의 벡터 형태로 전달됨
encoder와 decoder는 둘 다 recurrent neural networks(RNN)을 이용하는 경우가 많음

context는 float으로 이루어진 하나의 벡터이며, 그 크기는 모델을 처음 설정할 때 원하는 값으로 설정할 수 있음

보통 encoder RNN의 hidden unit 개수로 설정

실제 연구에서는 256, 512, 1024와 같은 숫자를 이용

Seq2seq 모델 디자인을 보면 하나의 RNN은 한 타임 스텝마다 두개의 입력을 받음

1. sequence의 한 아이템
2. 그전 스텝에서의 RNN hidden state

이 두 입력들은 RNN에 들어가기 전에 꼭 vector로 변환 되어야 하는데 이를 위해 "word embedding"을 사용

임베딩을 통해 단어들은 벡터 공간에 투영되고, 그 공간에서 단어 간 다양한 의미와 관련된 정보들을 얻어낼 수 있음

보통의 경우 200 혹은 300 차원의 임베딩 벡터를 이용


seq2seq 모델의 encoder와 decoder는 모두 RNN이며, RNN은 한번 아이템을 처리할 때마다 새로 들어온 아이템을 이용해 그의 hidden state를 업데이트 함

결과적으로 hidden state는 encoder가 보는 입력 시퀀스 내의 모든 단어에 대한 정보를 담게 됨

모델의 입력이된 문장의 마지막 단어의 hidden state는 우리가 decoder에 넘겨주는 context가 됨

![seq2seq model step](/img/2020-07-16/seq2seq_model_step.png)

decoder에도 hidden states가 존재하며 스텝마다 업데이트를 수행함


## Transformer


**`Transformer 모델의 핵심은 multi-head self-attention을 이용해 sequential computation을 줄여 더 많은 부분을 벙렬처리가 가능하게 만들면서 동시에 더 많은 단어들 간 dependency를 모델링한다는 것`**

Transformer는 Attention 개념을 활용한 모델로 attention을 학습하여 학습 속도를 크게 향상시킨 모델

이 모델은 몇몇의 테스크에서 기존의 seq2seq를 활용한 구글신경망 번역 시스템보다 좋은 성능을 보임

하지만 가장 큰 장점은 병렬처리에 관한 부분이며 Google Cloud는 Cloud TPU를 쓸 때 Transformer 모델을 기준으로 쓸 것을 추천함

transformer 모델의 내부에는 encoder와 decoder 그리고 그 사이를 이어주는 connection들로 이루어짐

encoding 부분은 여러 개의 encoder를 쌓아 올려 만든 것(논문에서는 6개 layer를 사용하였으나, 세팅은 변경 가능)

decoding 부분은 encoding 부분과 동일한 개수만큼의 decoder를 쌓은 것

encoder들은 모두 정확히 똑같은 구조를 가지고 있으나 그들 간에 같은 weight를 공유하진 않음

하나의 encoder 안에는 두 개의 sub-layer(Feed Forward Neural Network와 Self-Attention Layer)로 구성되어 있음

encoder로 들어온 입력은 먼저 self-attention layer를 지나가게 되는데, 이 때 encoder는 하나의 특정한 단어를 encode하기 위해 입력 내의 모든 다른 단어들과의 관계를 살펴봄

self-attention layer의 출력은 다시 feed-forward 신경망으로 들어가고, 똑같은 feed-forward 신경망이 각 위치의 단어마다 독립적으로 적용돼 출력을 만듬

decoder도 encoder의 두 layer를 모두 가지고 있으나 그 두 층 사이에 seq2seq 모델의 attention과 비슷한 encoder-decoder attention이 포함됨

이 layer는 decoder가 입력 문장 중에서 각 타임 스텝과 가장 관련 있는 부분에 집중할 수 있도록 해줌

Transformer의 입력이 되는 단어들은 먼저 512차원의 벡터로 임베딩 되며, 이 과정은 가장 밑단의 encoder에서만 일어남

즉 모든 encoder 들은 크기 512의 벡터 리스트를 입력으로 받고, 그 중 가장 밑단의 encoder의 경우 word embedding이 되고, 다른 encoder들에서는 바로 전의 encoder의 출력을 입력으로 받게 됨

이때 벡터 리스트의 크기는 하이퍼 파라미터로서 우리가 마음대로 정할 수 있고, 간단하게 생각하면 사용할 학습 데이터 셋에서 가장 긴 문장의 길이로 둘 수 있음


입력의 각 위치에 잇는 단어는 단어만의 path를 통해 encoder에서 흘러간다는 transformer 모델의 주요 성질을 볼 수 있음

self-attention 층에서 단어의 위치에 따른 path들 사이에는 dependency가 존재

반면 feed forward 층은 이러한 의존성이 없기 때문에 feed-forward layer 내 다양한 path들은 병렬처리가 가능함

## Encoder detail

encoder는 입력으로 벡터들의 리스트를 받는데, 이 리스트를 먼저 self-attention layer에, 그 다음으로 feed-forward 신경망에 통과시키고 그 결과물을 그 다음 encoder에 전달함

각 위치의 단어들은 각기 다른 self-encoding 과정을 거치고, 그 후 모두에게 같은 과정인 feed-forward 신경망을 거침

self-attention 계산의 가장 첫 단계는 encoder에 입력된 벡터들(이 경우 각 단어의 embedding 벡터)에게서 부터 각 3개의 벡터를 만들어냄

각 단어에 대해서 Query 벡터, Key 벡터 그리고 Value 벡터를 생성

이 벡터들은 입력 벡터에 대해서 세 개의 학습 가능한 행렬들을 각각 곱함으로써 만들어짐

새로운 벡터들은 기존의 벡터들 보다 더 작은 사이즈를 가짐
기존의 입력 벡터가 512의 크기라면 새로운 벡터들은 크기가 64가 됨
하지만 반드시 더 작아야하는 것은 아니며 이것은 multi-head attention의 계산 복잡도를 일정하게 만들고자 내린 구조적인 선택

query, key, value 벡터는 attention에 대해서 생각하고 계산하려할 때 도움이 되는 추상적인 개념

self-attention 계산의 두 번째 스텝은 점수를 계산하는 것

"Thinking"이라는 단어가 첫 번째로 들어왔을 때 우리는 이 단어와 입력 문장 속의 다른 모든 단어들에 대해서 각각 점수를 계산하여야 함

이 점수는 현재 위치의 이 단어를 encode할 때 다른 단어들에 대해서 얼마나 집중해야 할지를 결정

이 점수는 현재 단어의 query vector와 점수를 매기려하는 다른 위치에 있는 단어의 key vector의 내적으로 계산됨

첫 번째 점수는 q1과 k1의 내적, 두 번째 점수는 q1과 k2의 내적이 됨

세 번째와 네 번째 단계는 이 점수들을 8로 나누는 것인데, 이 8이란 숫자는 key 벡터의 사이즈인 64의 제곱근

![Self-Attention score 계산](/img/2020-07-16/attention_score_calculation.png)


나눗셈을 통해 더 안정적인 gradient를 가지게 되고, 그리고난 다음은 이 값을 softmax 계산을 통과시켜 모든 점수들을 양수로 만들고 그 합을 1로 만듦

이 softmax 점수는 현재 위치의 단어의 encoding에 있어서 얼마나 각 단어들의 표현이 들어갈 것인지를 결정
당연하게 현재 위치의 단어가 가장 높은 점수를 가지지만 가끔은 현재 단어에 관련이 잇는 다른 단어에 대한 정보가 들어가는 것이 도움이 됨

그 다음으로는 이제 입력의 각 단어들의 value 벡터에 이 점수를 곱함

이것은 우리가 집중하고 싶은 관련 단어들은 남겨두고 관련이 없는 단어들은 0.001과 같은 작은 숫자(점수)를 곱해 없애버리기 위함

마지막으로는 이 점수로 곱해진 weighted value벡터들을 다 합해버리는데 이 단계의 출력이 바로 현재 위치에 대한 self-attention layer의 출력이 됨

출력으로 나온 겨로가 벡터는 feed-forward 신경망으로 보내게 됨

실제 구현에서는 빠른 속도를 위해 모든 과정들이 벡터가 아닌 행렬의 형태로 진행됨

## Self Attention 의 행렬 계산


![행렬 형태로 표현한 self-attention 계산](/img/2020-07-16/attention_matrix_calculation.png)


## The Beast With Many Heads

이 논문은 self-attention layer에 "multi-headed" attention이라는 메커니즘을 더해 더욱더 개선함

논문에서는 2가지 방법으로 성능을 향상시킴

1. 모델이 다른 위치에 집중하는 능력을 확장시킴

    이것은 기존 attention이 자기 자신에게만 높은 점수를 주는 문제를 개선하여 "그 동물은 길을 건너지 않았다 왜냐하면 그것은 너무 피곤했기 때문이다"와 같은 문장에서 "그것"이 무엇을 가리키는지를 알아내는데 유용함

2. attention layer가 여러 개의 "representation 공간"을 가지게 해줌

    multi-headed attention을 이용함으로써 우리는 어러 개의 query/key/value weight 행렬을 가지게 됨

    논문에서는 8개의 attention heads를 가지므로 각 encoder/decoder마다 이런 8개의 세트를 가지게 됨

    이 각각의 qeury/key/value set은 랜덤으로 초기화되어 학습되고, 학습이 된 후에는 각각의 세트는 입력 벡터들에 곱해져 벡터들을 각 목적에 맞게 투영시킴

    이러한 세트가 여러개 있따는 것은 각 벡터들을 각각 다른 representation 공간으로 나타낸다는 것을 의미 


## Positional Encoding을 이용한 시퀀스의 순서 나타내기

이제까지 설명한 Transformer 모델의 한 가지 문제점은 입력 문장에서 단어들의 순서에 대해서 고려하고 있지 않다는 점

이것을 추가하기 위해서, Transformer 모델은 각각의 입력 embedding에 "positional encoding"이라고 불리는 하나의 벡터를 추가함

이 벡터는 모델이 학습하는 특정한 패턴을 따르는데, 이러한 패턴은 모델이 각 단어의 위치와 시퀀스 내의 다른 단어 간의 위치 차이에 대한 정보를 알 수 있게 해줌

이 벡터가 추가된 배경은 이 값들을 단어들의 embedding에 추가하는 것이 query/key/value 벡터들로 나중에 투영되었을 때 단어들 간의 거리를 늘릴 수 있다는 점

모델에게 단어의 순서에 대한 정보를 주기 위해 위치 별로 특정한 패턴을 따르는 positional encoding벡터들을 추가


## Transformer의 학습


모델의 최종 출력 벡터에 속한 값들을 보면 다른 단어들이 최종 추력이 될 가능성이 거의 없다해도 모든 단어가 0보다는 조금식 더 큰 확률을 가짐

-> 학습과정을 도와주는 softmax layer의 매우 유용한 성질

모델은 한 타임 스텝 당 하나의 벡터를 출력하기 때문에 우리는 모델이 가장 높은 확률을 가지는 하나의 단어만 저장하고 나머지는 버린다고 생각하기 쉬움

이러한 방식은 greedy decoding이라고 부르는 한 가지 방법일뿐 다른 방법들도 존재

예를 들어 가장 확률이 높은 두개의 단어를 저장하고, 모델을 두 번 돌림
한번은 첫 번째 출력이 I라고 가정하고 다른 한번은 student라고 가정했을 때, 두 번째 출력을 생성해보는 것

이렇게 해서 나온 결과 중에서 첫 번째와 두 번째 출력 단어를 동시에 고려했을 때 더 낮은 에러를 보이는 결과의 첫 번째 단어가 실제 출력으로 선택됨

이 과정을 두 번째, 세 번째, 그리고 마지막 다임 스텝까지 반복해 나감

이러한 방법을 "beam search"라고 부르고, 고려하는 단어의 수를 beam size, 고려하는 미래 출력 개수를 top_beams라고 함
위의 예제에서는 두개의 단어를 저장했으므로 beam size가 2이며, 첫 번째 출력을 위해 두 번째 스텝의 출력까지 고려했으므로 top_beams 또한 2인 beam search를 수행한 것

위의 parameter들은 학습 전에 설정할 수 있는 hyperparameter에 해당



## REFERENCE

https://nlpinkorean.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/